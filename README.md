<p align="right">
  Language Switch / è¯­è¨€é€‰æ‹©ï¼š
  <a href="./README.zh-CN.md">ðŸ‡¨ðŸ‡³ ä¸­æ–‡</a> | <a href="./README.md">ðŸ‡¬ðŸ‡§ English</a>
</p>

**INTRODUCTION**
---

This repository contains a demonstration of **binary classification using
the [SelmaNajih001/FinancialClassification](https://huggingface.co/datasets/SelmaNajih001/FinancialClassification)
dataset from Hugging Face**. The main goal is to train
a **[pretrained Transformer-based model](https://huggingface.co/google-bert/bert-base-chinese)** that can predict
whether a stock's price will go **up** or **down** based on a natural language description of a financial event.

We treat this as a **binary classification task**:

- **Input**: `Reasons` â€” a text description of an event or reason that impacted the stock market
- **Label**: Derived from `PriceVariation`
    - If `PriceVariation > 0` â†’ **Up (1)**
    - If `PriceVariation < 0` â†’ **Down (0)**

This task is useful for learning how to apply Hugging Faceâ€™s Transformers and Trainer APIs to real-world financial text
data and binary classification.


**DATA DESCRIPTION**
---

We use the `SelmaNajih001/FinancialClassification` dataset available on Hugging Face Datasets.

+ Fields
    - `Date`(`string`): The date when the financial event occurred
    - `PriceVariation`(`float`): The percentage change in stock price after the event
    - `Stock`(`string`): The name of the stock/company
    - `Reasons`(`string`): A textual description of the event or reason tied to the price variation |

+ Task Setup
    - `Reasons` is used as the **input text** for the model.
    - We derive a **binary label** from `PriceVariation`:
        - Positive values â†’ label = 1 (price up)
        - Negative values â†’ label = 0 (price down)
+ Notes
    - The dataset is not balanced by default, and it may require preprocessing or sampling for best model performance.
    - This dataset is well-suited for practicing NLP classification with Hugging Face Transformers, including data
      loading with `datasets`, tokenization, model training with `Trainer`, and evaluation.

+ Path
    - You can locate the data in you Mac using the command `~/.cache/huggingface/datasets` when you click the menu bar
      button, if you plan to find the downloaded data locally. Or, you can run the command
      `open ~/.cache/huggingface/datasets` using terminal.

**QUICK START**
---

1. Clone the repository to your local machine.
2. Install the required dependencies with the command `pip install -r requirements.txt`.
3. Run the application with the command `streamlit run main.py`.
4. You can also try the application by visiting the following
   link:  
   [![Static Badge](https://img.shields.io/badge/Open%20in%20Streamlit-Daochashao-red?style=for-the-badge&logo=streamlit&labelColor=white)](https://stock-reasons-pred.streamlit.app/)

**PRIVACY NOTICE**
---

This application is designed to process the data you provide to generate customized suggestions and results. Your
privacy is paramount.

**We do not collect, store, or transmit your personal information or data.** All processing occurs locally on your
device (in your browser or runtime environment), and **no data is ever sent to an external server or third party.**

- **Local Processing:** Your data never leaves your device. The entire analysis and generation process happens locally.
- **No Data Retention:** Since no data is transmitted, none is stored on any server. Closing the application typically
  clears any temporary local data.
- **Transparency:** The entire codebase is open source. You are encouraged to review the [code](./) to verify how your
  data is handled.

In summary, you maintain full control and ownership of your data at all times.

**LICENCE**
---
This project is open source and available under the **[BSD-3-Clause Licence](LICENCE)**.

In simple terms, this is a very permissive licence that allows you to freely use this code for almost any purpose,
including in proprietary projects, as long as you include the original copyright and licence notice.

Feel free to fork, modify, and build upon this work! We simply ask that you give credit where credit is due.

**ENVIRONMENT SETUP**
---
This project uses **Python 3.12** and [uv](https://docs.astral.sh/uv/) for fast dependency management and virtual
environment handling. The required Python version is automatically detected from the [.python-version](.python-version)
file.

1. **Installing uv**:  
   If you don't have `uv` installed, you can install it using the following command:
    ```bash
    curl -LsSf https://astral.sh/uv/install.sh | sh
    # This installation method works on macOS and Linux.
    ```
   Alternatively, you can install it by running the following PowerShell command:
    ```bash
    powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
    # This installation method works on Windows.
    ```

   **ðŸ’¡ Recommended**: For the best experience, install `uv` as a standalone tool. Avoid installing it within `pip` or
   `conda` environments to prevent potential dependency conflicts.

2. **Adding Dependencies**:

- To add a main (production) dependency:
    ```bash
    uv add <package_name>
    # This automatically updates pyproject.toml and installs the package
    ```
- To add a development dependency:
    ```bash
    uv add <package_name> --group dev
    # Example: uv add ruff --group dev
    # This adds the package to the [project.optional-dependencies.dev] section automatically
    ```
- To add other types of optional dependencies (e.g., test, docs):
    ```bash
    uv add <package_name> --group test
    uv add <package_name> --group docs
    ```
- To import dependencies from a `requirements.txt` file:
    ```bash
    uv add -r requirements.txt
    # This reads packages from requirements.txt and adds them to pyproject.toml
    ```
- Generate a `requirements.txt` file from the current dependencies:
    ```bash
    # This exports all dependencies, including optional ones, to requirements-all.txt
    uv export > requirements.txt
    # This exports only main dependencies to requirements-main.txt
    uv export --format requirements.txt --no-hashes --no-annotate -o requirements.txt
    ```

3. Removing Dependencies

- To remove a main (production) dependency:
    ```bash
    uv remove <package_name>
    # This automatically updates pyproject.toml and removes the package
    ```
- To remove a development dependency:
    ```bash
    uv remove <package_name> --group dev
    # Example: uv remove ruff --group dev
    # This removes the package from the [project.optional-dependencies.dev] section
    ```
- To remove other types of optional dependencies:
    ```bash
    uv remove <package_name> --group test
    uv remove <package_name> --group docs
    ```

4. **Managing the Environment**

- After using add/remove commands, sync the environment:
    ```bash
    uv sync
    ```

**CHANGELOG**
---
This project uses [git-changelog](https://github.com/pawamoy/git-changelog) to automatically generate and maintain a
changelog based on [Conventional Commits](https://www.conventionalcommits.org/).

1. **Installation**
   ```bash
   pip install git-changelog
   # or use uv to add it as a development dependency
   uv add git-changelog --group dev
   ```
2. **Verify Installation**
   ```bash
   pip show git-changelog
   # or check the version specifically
   pip show git-changelog | grep Version
   ```
3. **Configuration**
   Ensure you have a properly configured `pyproject.toml` file at the project root. The configuration should specify
   Conventional Commits as the changelog style. Here is an example configuration:
   ```toml
   [tool.git-changelog]
   version = "0.1.0"
   style = "conventional-commits"
   output = "CHANGELOG.md"
   ```
4. **Generate Changelog**
   ```bash
   git-changelog --output CHANGELOG.md
   # Or use uv to run it if installed as a dev dependency
   uv run git-changelog --output CHANGELOG.md
   ```
   This command creates or updates the `CHANGELOG.md` file with all changes based on your git history.
5. **Push Changes**
   ```bash
   git push origin main
   ```
   Alternatively, use your IDE's Git interface (e.g., `Git â†’ Push` in many editors).
6. **Note**:

- The changelog is automatically generated from your commit messages following the Conventional Commits specification.
- Run the generation command whenever you want to update the changelog, typically before a release or after significant
  changes.

**LARGE FILE STORAGE (LFS)**
---
This project uses Git Large File Storage (LFS) to manage large files, such as datasets, models, and binary files. The
instructions as follows are only used to upload the large file to the remote repository.

1. Install Git LFS by running the command:
    ```bash
    # For MacOS using Homebrew
    brew install git-lfs
    ```
2. Initialise Git LFS in the repository by running the command **ONCE**:
    ```bash
    git lfs install
    ```
3. Track the large files by using the command:
    ```bash
    git lfs track "*.pth"
    git lfs track "*.safetensors"
    ```
   You can replace `*.pth` with the appropriate file extension.
4. Add the `.gitattributes` file to version control using the UI interface or running the command:
    ```bash
    git add .gitattributes
    ```
5. Add the `unet4.pth` file to version control using the UI interface or
   running the command:
    ```bash
    git add models/model.pth
    git add models/model.safetensors
    ```
6. Commit the changes using the UI interface or running the command:
    ```bash
    git commit -m "Track large files with Git LFS"
    ```
7. List all files being tracked by the Git LFSUse command:
    ```bash
    git lfs ls-files
    ```
8. Push the changes to the remote repository using the UI interface or running the command:
    ```bash
    git push origin main
    ```
9. If you change the name of remote while initialising the repository, you need to change the `origin` to your remote
   name, such as `GitHub` or `xxx`, in the command `git push -u GitHub main` or `git push -u GitHub main`. Besides, if
   you change the branch name, you also need to change the `main` to your branch name, such as `master` or `xxx`, in the
   command `git push -u GitHub master` or `git push -u xxx master`. Therefore, it is better to keep the default names of
   remote and branch.
10. If you fail to push the large files, you might have used 2FA authentication. The normal push of the button of the
    UI interface is invalid. You can try to use a **personal access token (PAT)** instead of accessing the GitHub
    repository. If you have had the token, run the command `git push origin main` first. Then, enter the `username` and
    the `token` as the password.
11. When you push with `username` and `token` successfully first, you can continue to use the button of the UI interface
    to push the changes.
12. If you use `username` and `password` to initialise the repository, and you use the `personal access token (PTA)` to
    push the large files, you might fail to push the future changes with the `push` button of the UI. In this case, you
    can close the LFS push function by running the following command:
    ```bash
    git config lfs.<remote-url>/info/lfs.locksverify
    ```
    Then, you can use the `push` button of the UI to push the changes.
13. You must **install Git LFS locally** before you clone the repository if you plan to get the
    **full size of the data**. Otherwise, you will only get the pointer files. you can run the following command to
    install Git LFS:
    ```bash
    git lfs uninstall
    ```
14. (Optional) If you have already cloned the repository without Git LFS installed, you can run the following command to
    fetch the actual large files:
    ```bash
    git lfs pull
    ```